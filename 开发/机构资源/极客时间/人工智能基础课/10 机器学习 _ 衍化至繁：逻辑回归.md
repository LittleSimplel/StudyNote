<audio title="10 机器学习 _ 衍化至繁：逻辑回归" src="https://static001.geekbang.org/resource/audio/91/c8/91ac5e7a8fa259d498160fec8c1c68c8.mp3" controls="controls"></audio> 
<p>周四我和你分享了机器学习中的朴素贝叶斯分类算法，这一算法解决的是将连续取值的输入映射为离散取值的输出的分类问题。朴素贝叶斯分类器是一类<strong>生成模型</strong>，通过构造联合概率分布$P(X, Y)$实现分类。如果换一种思路，转而用<strong>判别模型</strong>解决分类问题的话，得到的算法就是“<strong>逻辑回归</strong>”。</p>
<p><strong>虽然顶着“回归”的名号，但逻辑回归解决的却是实打实的分类问题</strong>。之所以取了这个名字，原因在于它来源于对线性回归算法的改进。通过引入单调可微函数$g(\cdot)$，线性回归模型就可以推广为$y = g ^ {-1} (\mathbf{w} ^ T \mathbf{x})$，进而将线性回归模型的连续预测值与分类任务的离散标记联系起来。当$g(\cdot)$取成对数函数的形式时，线性回归就演变为了逻辑回归。</p>
<!-- [[[read_end]]] -->
<p>在最简单的二分类问题中，分类的标记可以抽象为0和1，因而线性回归中的实值输出需要映射为二进制的结果。逻辑回归中，实现这一映射是对数几率函数，也叫Sigmoid函数</p>
<p> $$ y = \dfrac{1}{1 + e ^ {-z}} = \dfrac{1}{1 + e ^ {- (\mathbf{w} ^ T \mathbf{x})}} $$ </p>
<p><strong>之所以选择对数几率函数，是因为它具备良好的特性</strong>。</p>
<p>首先，对数几率函数能够将线性回归从负无穷到正无穷的输出范围压缩到(0, 1)之间，无疑更加符合对二分类任务的直观感觉。</p>
<p>其次，当线性回归的结果$z = 0$时，逻辑回归的结果$y = 0.5$，这可以视为一个分界点：当$z &gt; 0$时，$y &gt; 0.5$，此时逻辑回归的结果就可以判为正例；当$z &lt; 0$时，$y &lt; 0.5$，逻辑回归的结果就可以判为反例。</p>
<p>显然，对数几率函数能够在线性回归和逻辑回归之间提供更好的可解释性。这种可解释性可以从数学的角度进一步诠释。</p>
<p>如果将对数几率函数的结果$y$视为样本$\mathbf{x}$作为正例的可能性，则$1 - y$就是其作为反例的可能性，两者的比值$0 &lt; \dfrac{y}{1 -y} &lt; +\infty$称为几率，体现的是样本作为正例的相对可能性。如果对几率函数取对数，并将前文中的公式代入，可以得到 </p>
<p>$$ \ln \dfrac{y}{1 -y} = \mathbf{w} ^ T \mathbf{x} + b $$</p>
<p>由此可见，当利用逻辑回归模型解决分类任务时，线性回归的结果正是以对数几率的形式出现的。</p>
<p>归根结底，逻辑回归模型由条件概率分布表示 </p>
<p>$$ p(y = 1 | \mathbf{x} ) = \dfrac{e ^ {\mathbf{w} ^ T \mathbf{x} + b}}{1 + e ^ {\mathbf{w} ^ T \mathbf{x} + b}}$$</p>
<p>$$p(y = 0 | \mathbf{x} ) = \dfrac{1}{1 + e ^ {\mathbf{w} ^ T \mathbf{x} + b}} $$</p>
<p><strong>对于给定的实例，逻辑回归模型比较两个条件概率值的大小，并将实例划分到概率较大的分类之中</strong>。</p>
<p><strong>学习时，逻辑回归模型在给定的训练数据集上应用最大似然估计法确定模型的参数</strong>。对给定的数据集${ (\mathbf{x}_i, y_i)}$，逻辑回归使每个样本属于其真实标记的概率最大化，以此为依据确定$\mathbf{w}$的最优值。由于每个样本的输出$y_i$都满足两点分布，且不同的样本之间相互独立，因而似然函数可以表示为</p>
<p>$$ L(\mathbf{w} | \mathbf{x}) = \Pi_{i = 1}^N [p(y = 1 | \mathbf{x}_i, \mathbf{w})] ^ {y_i} $$</p>
<p>$$\cdot [1 - p(y = 1 | \mathbf{x}_i, \mathbf{w})] ^ {1 - y_i} $$ </p>
<p>利用对数操作将乘积转化为求和，就可以得到对数似然函数</p>
<p> $$ \log L(\mathbf{w} | \mathbf{x}) = \sum\limits_{i = 1}^N y_i \log [p(y = 1 | \mathbf{x}_i, \mathbf{w})] $$</p>
<p>$$+ (1 - y_i) \log [1 - p(y = 1 | \mathbf{x}_i, \mathbf{w})]$$ </p>
<p>由于单个样本的标记$y_i$只能取得0或1，因而上式中的两项中只有一个有非零的取值。将每个条件概率的对数几率函数形式代入上式，经过化简可以得到</p>
<p>$$ \log L(\mathbf{w} | \mathbf{x}) = \sum\limits_{i = 1}^N y_i \cdot (\mathbf{w} ^ T \mathbf{x}_i) $$</p>
<p>$$- \log (1 + e ^ {\mathbf{w} ^ T \mathbf{x}_i}) $$ </p>
<p><strong>寻找以上函数的最大值就是以对数似然函数为目标函数的最优化问题，通常通过“梯度下降法”或拟“牛顿法”求解</strong>。</p>
<p>当训练数据集是从所有数据中均匀抽取且数量较大时，以上结果还有一种信息论角度的阐释方式：<strong>对数似然函数的最大化可以等效为待求模型与最大熵模型之间KL散度的最小化</strong>。这意味着最优的估计对参数做出的额外假设是最少的，这无疑与最大熵原理不谋而合。</p>
<p>从数学角度看，线性回归和逻辑回归之间的渊源来源于非线性的对数似然函数；而从特征空间的角度看，两者的区别则在于数据判定边界的变化。判定边界可以类比为棋盘上的楚河汉界，边界两侧分别对应不同类型的数据。</p>
<p>以最简单的二维平面直角坐标系为例。受模型形式的限制，利用线性回归只能得到直线形式的判定边界；逻辑回归则在线性回归的基础上，通过对数似然函数的引入使判定边界的形状不再受限于直线，而是推广为更加复杂的曲线形式，更加精细的分类也就不在话下。</p>
<p>逻辑回归与线性回归的关系称得上系出同门，与朴素贝叶斯分类的关系则是殊途同归。两者虽然都可以利用条件概率$P(Y|X)$完成分类任务，实现的路径却截然不同。</p>
<p>朴素贝叶斯分类器是生成模型的代表，其思想是先由训练数据集估计出输入和输出的联合概率分布，再根据联合概率分布来生成符合条件的输出，$P(Y|X)$以后验概率的形式出现。</p>
<p>逻辑回归模型则是判别模型的代表，其思想是先由训练数据集估计出输入和输出的条件概率分布，再根据条件概率分布来判定对于给定的输入应该选择哪种输出，$P(Y|X)$以似然概率的形式出现。</p>
<p><strong>即便原理不同，逻辑回归与朴素贝叶斯分类器在特定的条件下依然可以等效</strong>。用朴素贝叶斯分类器处理二分类任务时，假设对每个$\mathbf{x}_i$，属性条件概率$p(\mathbf{x}_i | Y = y_k)$都满足正态分布，且正态分布的标准差与输出标记$Y$无关，那么根据贝叶斯定理，后验概率就可以写成 </p>
<p>$$ p(Y = 0 | X) =$$</p>
<p>$$ \dfrac{p(Y = 0) \cdot p(X | Y = 0)}{p(Y = 1) \cdot p(X | Y = 1) + p(Y = 0) \cdot p(X | Y = 0)} $$ </p>
<p> $$ = \dfrac{1}{1 + \exp (\ln \frac{p(Y = 1) \cdot p(X | Y = 1)}{p(Y = 0) \cdot p(X | Y = 0)})} $$ </p>
<p>根据朴素贝叶斯方法的假设，类条件概率可以表示为属性条件概率的乘积，因而令$p(Y = 0) = p_0$并将满足正态分布的属性条件概率$p(\mathbf{x}_i | Y = y_k)$代入以上表达式中，经过一番计算就可以得到</p>
<p> $$ p(Y = 0 | X) = $$</p>
<p>$$\dfrac{1}{1 + \exp(\ln \frac{1 - p_0}{p_0} + \sum\limits_i(\frac{\mu_{i1} - \mu_{i0}}{\sigma_i^2} X_i + \frac{\mu_{i0} ^ 2 - \mu_{i1} ^ 2}{2\sigma_i^2}))} $$ </p>
<p>不难看出，上式的形式和逻辑回归中条件概率$p(y = 0 | \mathbf{x} )$的形式是完全一致的，这表明朴素贝叶斯方法和逻辑回归模型学习到的是同一个模型。实际上，在$p(x | Y)$的分布属于指数分布族这个更一般的假设下，类似的结论都是成立的。</p>
<p>说完了联系，再来看看区别。<strong>两者的区别在于当朴素贝叶斯分类的模型假设不成立时，逻辑回归和朴素贝叶斯方法通常会学习到不同的结果</strong>。当训练样本数接近无穷大时，逻辑回归的渐近分类准确率要优于朴素贝叶斯方法。而且逻辑回归并不完全依赖于属性之间相互独立的假设，即使给定违反这一假设的数据，逻辑回归的条件似然最大化算法也会调整其参数以实现最大化的数据拟合。相比之下，逻辑回归的偏差更小，但方差更大。</p>
<p>除此之外，<strong>两者的区别还在于收敛速度的不同</strong>。逻辑回归中参数估计的收敛速度要慢于朴素贝叶斯方法。当训练数据集的容量较大时，逻辑回归的性能优于朴素贝叶斯方法；但在训练数据稀缺时，两者的表现就会发生反转。</p>
<p>二分类任务只是特例，更通用的情形是多分类的问题，例如手写数字的识别。要让逻辑回归处理多分类问题，就要做出一些改进。</p>
<p><strong>一种改进方式是通过多次二分类实现多个类别的标记</strong>。这等效为直接将逻辑回归应用在每个类别之上，对每个类别都建立一个二分类器。如果输出的类别标记数目为$m$，就可以得到$m$个针对不同标记的二分类逻辑回归模型，而对一个实例的分类结果就是这$m$个分类函数中输出值最大的那个。在这种方式中，对一个实例执行分类需要多次使用逻辑回归算法，其效率显然比较低下。</p>
<p><strong>另一种多分类的方式通过直接修改逻辑回归输出的似然概率，使之适应多分类问题，得到的模型就是Softmax回归</strong>。Softmax回归给出的是实例在每一种分类结果下出现的概率</p>
<p> $$ p(Y = k | x) = \dfrac{e ^ {\mathbf{w_k} ^ T \mathbf{x}}}{\sum\limits_{k = 1}^K e ^ {\mathbf{w_k} ^ T \mathbf{x}}} $$ </p>
<p>式中的$\mathbf{w_k}$代表和类别$k$相关的权重参数。Softmax回归模型的训练与逻辑回归模型类似，都可以转化为通过梯度下降法或者拟牛顿法解决最优化问题。</p>
<p>虽然都能实现多分类的任务，但两种方式的适用范围略有区别。当分类问题的所有类别之间明显互斥，即输出结果只能属于一个类别时，Softmax分类器是更加高效的选择；当所有类别之间不存在互斥关系，可能有交叉的情况出现时，多个二分类逻辑回归模型就能够得到多个类别的标记。</p>
<p>今天我和你分享了机器学习基本算法之一的逻辑回归方法的基本原理，其要点如下：</p>
<ul>
<li>逻辑回归模型是对线性回归的改进，用于解决分类问题；</li>
<li>逻辑回归输出的是实例属于每个类别的似然概率，似然概率最大的类别就是分类结果；</li>
<li>在一定条件下，逻辑回归模型与朴素贝叶斯分类器是等价的；</li>
<li>多分类问题时可以通过多次使用二分类逻辑回归或者使用Softmax回归解决。</li>
</ul>
<p>前文对逻辑回归的分析都是在概率理论的基础上完成的。但在二分类任务中，逻辑回归的作用可以视为是在平面直角坐标系上划定一条数据分类的判定边界。那么逻辑回归的作用能不能从几何角度理解，并推广到高维空间呢？</p>
<p>欢迎发表你的观点。</p>
<p><img src="https://static001.geekbang.org/resource/image/d8/aa/d81794d22373b75dd79da8655adacdaa.jpg" alt=""></p>
<p></p>
